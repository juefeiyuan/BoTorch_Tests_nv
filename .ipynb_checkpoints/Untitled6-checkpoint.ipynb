{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tempfile\n",
    "import copy\n",
    "import numpy          as np\n",
    "import numpy.random   as npr\n",
    "import scipy.linalg   as spla\n",
    "import scipy.stats    as sps\n",
    "import scipy.optimize as spo\n",
    "import multiprocessing\n",
    "import ast\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import numpy as np\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "#from pylab import *\n",
    "import random\n",
    "import warnings\n",
    "from torch.distributions import Normal\n",
    "import time\n",
    "\n",
    "\n",
    "import math\n",
    "import scipy.linalg as spla\n",
    "import scipy.optimize as spo\n",
    "import scipy.stats as sps\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import functools\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def target(X):\n",
    "    x1 = X[0]\n",
    "    x2 = X[1]\n",
    "    \n",
    "    result = (x2- (5.1/(4*math.pi*math.pi))*x1*x1 + (5/math.pi)*x1 - 6)**2 + 10*(1 - (1/(8*math.pi)))*torch.cos(x1) + 10\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims = 2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def train_GP_model(model, likelihood, train_x, train_y, training_iter = 150):\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "    ], lr=0.1)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "    #     print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "    #         i + 1, training_iter, loss.item(),\n",
    "    #         model.covar_module.base_kernel.lengthscale.item(),\n",
    "    #         model.likelihood.noise.item()\n",
    "    #     ))\n",
    "        optimizer.step()\n",
    "        \n",
    "    return model, likelihood\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def EI(x, train_y, model, likelihood):\n",
    "\n",
    "    x = x.astype(np.float32)\n",
    "    x = torch.from_numpy(x)\n",
    "    x = x.resize_((1,2))\n",
    "    x = x.cuda(device)\n",
    "    f_star = torch.min(train_y)\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    f_preds = model(x)\n",
    "    pos_mu = f_preds.mean\n",
    "    pos_var = f_preds.variance\n",
    "    pos_sigma = pos_var.clamp_min(1e-9).sqrt()\n",
    "\n",
    "\n",
    "    W = (f_star - pos_mu)/pos_sigma\n",
    "    normal = Normal(torch.zeros_like(W), torch.ones_like(W))\n",
    "    ucdf = normal.cdf(W)\n",
    "    updf = torch.exp(normal.log_prob(W))\n",
    "    ei = pos_sigma * (updf + W * ucdf)\n",
    "    result = -ei\n",
    "    result = result.cpu().detach().contiguous().double().clone().numpy()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def EIC(x, train_y, train_y_constraint_1, model, model_constraint_1, likelihood, likelihood_constraint_1, constrain_threshold, leq = True):\n",
    "    \n",
    "#     x = x.astype(np.float32)\n",
    "#     x = torch.from_numpy(x)\n",
    "#     x = x.resize_((1,2))\n",
    "\n",
    "#     if leq:\n",
    "#         f_star = torch.min(train_y[train_y_constraint_1 <= constrain_threshold])\n",
    "#     else:\n",
    "#         f_star = torch.min(train_y[train_y_constraint_1 > constrain_threshold])\n",
    "\n",
    "#     f_star = f_star.cpu()\n",
    "#     #     print('haha')\n",
    "#     x = x.cuda(cuda0)\n",
    "#     #print(x)\n",
    "#     model.eval()\n",
    "#     likelihood.eval()\n",
    "#     f_preds = model(x)\n",
    "#     pos_mu = f_preds.mean\n",
    "#     pos_var = f_preds.variance\n",
    "#     pos_sigma = pos_var.clamp_min(1e-9).sqrt()\n",
    "\n",
    "\n",
    "#     model_constraint_1.eval()\n",
    "#     likelihood_constraint_1.eval()\n",
    "#     f_preds_constraint_1 = model_constraint_1(x)\n",
    "#     pos_mu_c1 = f_preds_constraint_1.mean\n",
    "#     pos_var_c1 = f_preds_constraint_1.variance\n",
    "#     pos_sigma_c1 = pos_var_c1.clamp_min(1e-9).sqrt()\n",
    "\n",
    "\n",
    "#     W = (f_star - pos_mu)/pos_sigma\n",
    "#     normal = Normal(torch.zeros_like(W), torch.ones_like(W))\n",
    "#     ucdf = normal.cdf(W)\n",
    "#     updf = torch.exp(normal.log_prob(W))\n",
    "#     ei = pos_sigma * (updf + W * ucdf)\n",
    "\n",
    "\n",
    "#     W_constraint_1 = (constrain_threshold - pos_mu_c1)/pos_sigma_c1\n",
    "#     normal_constraint_1 = Normal(torch.zeros_like(W_constraint_1), torch.ones_like(W_constraint_1))\n",
    "#     ucdf_constraint_1 = normal_constraint_1.cdf(W_constraint_1)\n",
    "# #     print('ei', ei)\n",
    "# #     print('constraint cdf',ucdf_constraint_1)\n",
    "# #     print('w constraint', W_constraint_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     if leq:\n",
    "#         eic = ei*ucdf_constraint_1\n",
    "#     else:\n",
    "#         eic = ei*(1 - ucdf_constraint_1)\n",
    "\n",
    "#     result = -eic\n",
    "#     result = result.cpu().detach().contiguous().double().clone().numpy()\n",
    "#     # print('EIC is ', result)\n",
    "#     return result\n",
    "\n",
    "\n",
    "\n",
    "def next_sampling_point(EI, train_y, model, likelihood, bnds, opt_method, maxiter, dtype, device, nRestart =25):\n",
    "\n",
    "\n",
    "    min_value = 1000\n",
    "    point_to_sample = None\n",
    "\n",
    "\n",
    "    objective = functools.partial(EI, train_y = train_y,model = model, likelihood = likelihood)\n",
    "\n",
    "\n",
    "    for i in range(nRestart):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        initial_point = torch.rand(1, 2, dtype=dtype, device=device)*15 + torch.tensor([-5.0, 0.0], dtype=dtype, device=device)\n",
    "        initial_point = initial_point.cpu()\n",
    "        function_to_optimize = objective\n",
    "    #         print('haha1')\n",
    "        warnings.filterwarnings(\"error\")\n",
    "\n",
    "        res = spo.minimize(function_to_optimize, initial_point, method = optimize_method, \\\n",
    "                               options={'maxiter':maxiter, 'disp': False}, bounds = bnds)  \n",
    "            #         print('haha2')\n",
    "        if res.fun < min_value:\n",
    "            min_value = res.fun\n",
    "            point_to_sample = res.x   \n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "    point_to_sample = torch.from_numpy(point_to_sample)\n",
    "    point_to_sample = point_to_sample.type(dtype)\n",
    "    point_to_sample = point_to_sample.reshape(1,2)\n",
    "    point_to_sample = point_to_sample.cuda(device)\n",
    "    return point_to_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = next_sampling_point(EI, train_y, model, likelihood, bnds, opt_method, maxiter, dtype, device, nRestart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.8249, 0.1778]], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x.reshape(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-3138897b27ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "torch.tensor([new_x], dtype = dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.2738,  0.1477],\n",
       "        [ 3.0462,  7.8490],\n",
       "        [ 9.2881, 11.3382],\n",
       "        [-1.2175,  5.0481],\n",
       "        [-1.4686,  3.0192],\n",
       "        [-3.6309,  5.7903],\n",
       "        [ 7.4083, 10.5197],\n",
       "        [ 0.6641, 12.6279],\n",
       "        [-1.5338,  8.5984],\n",
       "        [ 1.2108, 11.6176]], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = target(new_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.4129], device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y.reshape(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.3596, 30.6739, 81.0571, 22.8156, 42.3037, 60.6851, 99.1541, 75.7463,\n",
       "        10.3765, 67.4815], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_x =  torch.cat((train_x, new_x), 0)\n",
    "train_y = torch.cat((train_y, new_y.reshape(1)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float\n",
    "nRestart = 1000\n",
    "optimize_method = 'SLSQP'\n",
    "maxiter = 10000\n",
    "d = 2\n",
    "training_iter = 150\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bnds = [np.array([-5., 10.]), np.array([ 0., 15.])]\n",
    "# bnds = bnds*num_data_points\n",
    "\n",
    "\n",
    "\n",
    "opt_method = 'SLSQP'\n",
    "\n",
    "#We obtain three random samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# hypers = {\n",
    "#     'covar_module.base_kernel.lengthscale': torch.tensor(0.3),\n",
    "# }\n",
    "\n",
    "train_x = torch.rand(10, 2, dtype=dtype, device=device)*15 + torch.tensor([-5.0, 0.0], dtype=dtype, device=device)\n",
    "\n",
    "train_y = torch.zeros(train_x.shape[0], dtype=dtype, device=device)\n",
    "for i in range(train_x.shape[0]):\n",
    "    train_y[i] = target(train_x[i,:])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration number is  0\n",
      "Best value is  tensor(10.2052, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "iterations = 1\n",
    "\n",
    "for i in range(iterations):\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(train_x, train_y, likelihood)\n",
    "    model = model.cuda(device)\n",
    "    likelihood = likelihood.cuda(device)\n",
    "    model, likelihood = train_GP_model(model, likelihood, train_x, train_y, training_iter)\n",
    "\n",
    "    new_x = next_sampling_point(EI, train_y, model, likelihood, bnds, opt_method, maxiter, dtype, device, nRestart)\n",
    "    new_y = target(new_x[0])\n",
    "    train_x =  torch.cat((train_x, new_x), 0)\n",
    "    train_y = torch.cat((train_y, new_y.reshape(1)), 0)\n",
    "    \n",
    "    print('The iteration number is ', i)\n",
    "    print('Best value is ', train_y.min())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.rand(10, 2, dtype=dtype, device=device)*15 + torch.tensor([-5.0, 0.0], dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.2738,  0.1477],\n",
       "        [ 3.0462,  7.8490],\n",
       "        [ 9.2881, 11.3382],\n",
       "        [-1.2175,  5.0481],\n",
       "        [-1.4686,  3.0192],\n",
       "        [-3.6309,  5.7903],\n",
       "        [ 7.4083, 10.5197],\n",
       "        [ 0.6641, 12.6279],\n",
       "        [-1.5338,  8.5984],\n",
       "        [ 1.2108, 11.6176]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = torch.zeros(train_x.shape[0], dtype=dtype, device=device)\n",
    "\n",
    "for i in range(Xsamples.shape[0]):\n",
    "    train_y[i] = target(train_x[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.3596, 30.6739, 81.0571, 22.8156, 42.3037, 60.6851, 99.1541, 75.7463,\n",
       "        10.3765, 67.4815], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "# model.initialize(**hypers)\n",
    "model = model.cuda(device)\n",
    "likelihood = likelihood.cuda(device)\n",
    "model, likelihood = train_GP_model(model, likelihood, train_x, train_y, training_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = np.array(target(new_x))\n",
    "\n",
    "new_y_constrain1 =  np.array(constrain_function_1(new_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.3596, 30.6739, 81.0571, 22.8156, 42.3037, 60.6851, 99.1541, 75.7463,\n",
       "        10.3765, 67.4815,  7.4129], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (11) must match the size of tensor b (14) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-ccbd490c5b1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mlikelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_GP_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mnew_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_sampling_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbnds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnRestart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-56e03a74e282>\u001b[0m in \u001b[0;36mtrain_GP_model\u001b[1;34m(model, likelihood, train_x, train_y, training_iter)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m# Calc loss and backprop gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mmll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;31m#     print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_validate_module_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, output, target, *params)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Get the log prob of the marginal distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Add additional terms (SGPR / learned inducing points, heteroskedastic likelihood models)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_covariance_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m# Repeat the covar to match the batch shape of diff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (11) must match the size of tensor b (14) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "\n",
    "for i in range(iterations):\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(train_x, train_y, likelihood)\n",
    "    model = model.cuda(device)\n",
    "    likelihood = likelihood.cuda(device)\n",
    "    model, likelihood = train_GP_model(model, likelihood, train_x, train_y, training_iter)\n",
    "\n",
    "    new_x = next_sampling_point(EI, train_y, model, likelihood, bnds, opt_method, maxiter, dtype, device, nRestart)\n",
    "    new_y = target(new_x[0])\n",
    "    train_x =  torch.cat((train_x, new_x), 0)\n",
    "    train_y = torch.cat((train_y, new_y.reshape(1)), 0)\n",
    "    \n",
    "    print('The iteration number is ', i)\n",
    "    print('Best value is ', train_y.min())\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = sys.argv[2]\n",
    "num_experiments = np.int(num_experiments)\n",
    "experiment_count = 0\n",
    "total_results = []\n",
    "GP_infer_total = []\n",
    "time_running = []\n",
    "\n",
    "while experiment_count < num_experiments:\n",
    "\n",
    "    Xsamples = lhsu(x_min, x_max, num_initial_points)\n",
    "\n",
    "\n",
    "    Ysamples = np.zeros((Xsamples.shape[0]))\n",
    "\n",
    "    for i in range(Xsamples.shape[0]):\n",
    "        Ysamples[i] = target(Xsamples[i,:])\n",
    "\n",
    "\n",
    "    #start = num_initial_points + 1\n",
    "\n",
    "    # Ysamples = np.asarray([Ysamples])\n",
    "    # Ysamples = Ysamples.T\n",
    "\n",
    "\n",
    "    Ysamples_constrain_1 = np.zeros((Xsamples.shape[0]))\n",
    "\n",
    "    for i in range(Xsamples.shape[0]):\n",
    "        Ysamples_constrain_1[i] = constrain_function_1(Xsamples[i,:])\n",
    "        \n",
    "    \n",
    "    if check_feasible_point(Xsamples, Ysamples, Ysamples_constrain_1, constrain_threshold, leq):\n",
    "        GP_infer_time = 0\n",
    "        start_time = time.time()\n",
    "        single_experiment_result = []\n",
    "        \n",
    "        \n",
    "        # maxiter = 2\n",
    "        # nRestart = 1\n",
    "        #global_minimum = target(np.array([(5-np.pi)/15,12.275/15]))\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "        iterations = 200\n",
    "        valid_count = 0\n",
    "        # start_time = time.time()\n",
    "        for i in range(iterations):\n",
    "            warnings.filterwarnings('ignore')\n",
    "            # Update Gaussian process with existing samples\n",
    "            print('iteration begin ', i)\n",
    "\n",
    "            Xsamples = Xsamples.astype(np.float32)\n",
    "            Ysamples = Ysamples.astype(np.float32)\n",
    "            Ysamples_constrain_1 = Ysamples_constrain_1.astype(np.float32)\n",
    "\n",
    "\n",
    "            train_x = torch.from_numpy(Xsamples)\n",
    "            train_y = torch.from_numpy(Ysamples)\n",
    "            train_y_constraint_1 = torch.from_numpy(Ysamples_constrain_1)\n",
    "            train_x = train_x.cuda(cuda0)\n",
    "            train_y = train_y.cuda(cuda0)\n",
    "            train_y_constraint_1 = train_y_constraint_1.cuda(cuda0)\n",
    "\n",
    "\n",
    "            try:\n",
    "            # Obtain next sampling point from the acquisition function (expected_improvement)\n",
    "                gp_infer_start_time = time.time()\n",
    "                print('GP begin')\n",
    "                likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "                model = ExactGPModel(train_x, train_y, likelihood)\n",
    "                model.initialize(**hypers)\n",
    "                model = model.cuda(cuda0)\n",
    "                likelihood = likelihood.cuda(cuda0)\n",
    "                model, likelihood = train_GP_model(model, likelihood, train_x, train_y, training_iter)\n",
    "                print('GP function done')\n",
    "                likelihood_constraint_1 = gpytorch.likelihoods.GaussianLikelihood()\n",
    "                model_constraint_1 = ExactGPModel(train_x, train_y_constraint_1, likelihood_constraint_1)\n",
    "                model_constraint_1.initialize(**hypers)\n",
    "                model_constraint_1 = model_constraint_1.cuda(cuda0)\n",
    "                likelihood_constraint_1 = likelihood_constraint_1.cuda(cuda0)\n",
    "                model_constraint_1, likelihood_constraint_1 = train_GP_model(model_constraint_1, likelihood_constraint_1,\\\n",
    "                                                                             train_x, train_y_constraint_1, training_iter)\n",
    "                print('GP constraint done')\n",
    "                gp_infer_end_time = time.time()\n",
    "                gp_infer_time = gp_infer_end_time - gp_infer_start_time\n",
    "                GP_infer_time = GP_infer_time + gp_infer_time\n",
    "            except:\n",
    "                print('GP fitting failed')\n",
    "                break\n",
    "        #         print('GP fitting failed')\n",
    "        #         Xsamples = Xsamples[:-1,:]\n",
    "        #         Ysamples = Ysamples[:-1,:]\n",
    "        #         Ysamples_constrain_1 = Ysamples_constrain_1[:-1,:]\n",
    "        #         continue\n",
    "\n",
    "\n",
    "\n",
    "        #     warnings.filterwarnings(\"error\")\n",
    "        #     print('bnds dimension ', len(bnds[0]))\n",
    "            try:\n",
    "                new_x = next_sampling_point(EIC, train_y, train_y_constraint_1, model, model_constraint_1, likelihood, \\\n",
    "                                likelihood_constraint_1, constrain_threshold, bnds, opt_method, maxiter, \\\n",
    "                                leq, nRestart)\n",
    "                # print('new x is ', new_x)\n",
    "            except:\n",
    "                print('sample next point failed')\n",
    "                break\n",
    "        #         print('sample next point failed')\n",
    "        #         Xsamples = Xsamples[:-1,:]\n",
    "        #         Ysamples = Ysamples[:-1,:]\n",
    "        #         Ysamples_constrain_1 = Ysamples_constrain_1[:-1,:]\n",
    "        #         continue\n",
    "\n",
    "\n",
    "            print('sample done')\n",
    "\n",
    "            try:\n",
    "                # Obtain next noisy sample from the objective function\n",
    "                new_y = np.array(target(new_x))\n",
    "\n",
    "                new_y_constrain1 =  np.array(constrain_function_1(new_x))\n",
    "                print('function value done')\n",
    "            except:\n",
    "                print(new_y)\n",
    "                print(new_y_constrain1)\n",
    "                print('funtion value failed')\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Add sample to previous samples\n",
    "            Xsamples = np.vstack((Xsamples, new_x))\n",
    "            Ysamples = np.hstack((Ysamples, new_y))\n",
    "            Ysamples_constrain_1 = np.hstack((Ysamples_constrain_1, new_y_constrain1))\n",
    "\n",
    "            print('append done')\n",
    "            if leq:\n",
    "                index_of_best = np.argmin(Ysamples[np.where(Ysamples_constrain_1 <= constrain_threshold)])\n",
    "                print('Best feasible pointtttttttttttttttttttttttttttt ', Xsamples[np.where(Ysamples_constrain_1 <= constrain_threshold)][index_of_best])\n",
    "                print('Best feasible valueeeeeeeeeeeeeeeeeeeeeeeeeeeee ', Ysamples[np.where(Ysamples_constrain_1 <= constrain_threshold)][index_of_best])\n",
    "                single_experiment_result.append(Ysamples[np.where(Ysamples_constrain_1 <= constrain_threshold)][index_of_best])\n",
    "            else:\n",
    "                index_of_best = np.argmin(Ysamples[np.where(Ysamples_constrain_1 <= constrain_threshold)])\n",
    "                print('Best feasible pointtttttttttttttttttttttttttttt ', Xsamples[np.where(Ysamples_constrain_1 > constrain_threshold)][index_of_best])\n",
    "                print('Best feasible valueeeeeeeeeeeeeeeeeeeeeeeeeeeee ', Ysamples[np.where(Ysamples_constrain_1 > constrain_threshold)][index_of_best])\n",
    "\n",
    "            valid_count = valid_count + 1\n",
    "#             print('valid count ', valid_count)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        single_exp_time = end_time - start_time\n",
    "        if valid_count == iterations:\n",
    "            GP_infer_total.append(GP_infer_time)\n",
    "            print('Single GP infer time is ', GP_infer_time)\n",
    "            time_running.append(single_exp_time)\n",
    "            print('Single running time is ', single_exp_time)\n",
    "        single_experiment_result = np.array(single_experiment_result)\n",
    "        total_results.append(single_experiment_result)\n",
    "        experiment_count = experiment_count + 1\n",
    "        print('experiment count ', experiment_count)\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "        \n",
    "total_results = np.array(total_results)\n",
    "result_file = 'EIC_gpu_results_' + sys.argv[1][-1] + '.txt'\n",
    "np.savetxt(result_file, total_results, fmt='%s')\n",
    "running_time_file = 'EIC_gpu_time_' + sys.argv[1][-1] + '.txt'\n",
    "np.savetxt(running_time_file , time_running)\n",
    "GP_time_file = 'EIC_gpu_GP_inference_time_' + sys.argv[1][-1] + '.txt'\n",
    "np.savetxt(GP_time_file, GP_infer_total)\n",
    "print('Average running time is ', np.mean(time_running))\n",
    "print('Average GP inference time is ', np.mean(GP_infer_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
